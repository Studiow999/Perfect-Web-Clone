"""
Main Orchestrator (nO)

å®ç° Claude Code çš„ä¸»ç¼–æ’å¾ªç¯ã€‚

æ ¸å¿ƒæµç¨‹ï¼ˆ6 é˜¶æ®µï¼‰ï¼š
1. æ¶ˆæ¯é¢„å¤„ç†
2. å‹ç¼©æ£€æŸ¥
3. ç³»ç»Ÿæç¤ºç”Ÿæˆ
4. å¯¹è¯æµç”Ÿæˆ
5. å·¥å…·æ‰§è¡Œ
6. ç»“æœæ”¶é›†

æ•´åˆæ‰€æœ‰æ ¸å¿ƒç»„ä»¶ï¼š
- h2A: AsyncMessageQueue
- wU2: MessageCompressor
- ga0: SystemPromptGenerator
- wu: StreamGenerator
- nE2: ConversationPipeline
- MH1: ToolExecutor
- UH1: ConcurrencyScheduler
"""

from __future__ import annotations
import logging
from typing import Dict, Any, List, Optional, AsyncGenerator
from datetime import datetime

from .constants import ExecutionContext, StreamEventType
from .message_queue import AsyncMessageQueue
from .compressor import MessageCompressor
from .prompt_generator import SystemPromptGenerator
from .stream_generator import StreamGenerator
from .conversation_pipeline import ConversationPipeline
from .tool_executor import ToolExecutor, ToolCall
from .concurrency_scheduler import ConcurrencyScheduler, TaskPriority

logger = logging.getLogger(__name__)


class MainOrchestrator:
    """
    ä¸»ç¼–æ’å™¨ï¼ˆnOï¼‰

    æ•´åˆæ‰€æœ‰æ ¸å¿ƒç»„ä»¶ï¼Œå®ç°å®Œæ•´çš„ Agent æ‰§è¡Œå¾ªç¯
    """

    def __init__(
        self,
        tools_registry: Dict[str, Any],
        tool_schemas: List[Dict[str, Any]],
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–ä¸»ç¼–æ’å™¨

        Args:
            tools_registry: å·¥å…·æ³¨å†Œè¡¨
            tool_schemas: å·¥å…· Schema åˆ—è¡¨
            api_key: Anthropic API Key
            base_url: API åŸºç¡€ URL
        """
        # åˆå§‹åŒ–æ‰€æœ‰æ ¸å¿ƒç»„ä»¶
        self.message_queue = AsyncMessageQueue()
        self.compressor = MessageCompressor()
        self.prompt_generator = SystemPromptGenerator()
        self.conversation_pipeline = ConversationPipeline(
            api_key=api_key,
            base_url=base_url,
        )
        self.tool_executor = ToolExecutor(
            tools_registry=tools_registry,
            tool_schemas=tool_schemas,
        )
        self.concurrency_scheduler = ConcurrencyScheduler()

        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            "iterations": 0,
            "messages_processed": 0,
            "tools_executed": 0,
            "compressions": 0,
        }

        logger.info("MainOrchestrator åˆå§‹åŒ–å®Œæˆ")

    async def run(
        self,
        user_message: str,
        context: ExecutionContext,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        max_iterations: int = 999999,  # No limit - Agent works until task is complete
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        è¿è¡Œä¸»ç¼–æ’å¾ªç¯

        è¿™æ˜¯æ ¸å¿ƒæ‰§è¡Œæµç¨‹ï¼ŒæŒ‰ç…§ Claude Code çš„ 6 é˜¶æ®µå¤„ç†ï¼š
        1. æ¶ˆæ¯é¢„å¤„ç†
        2. å‹ç¼©æ£€æŸ¥
        3. ç³»ç»Ÿæç¤ºç”Ÿæˆ
        4. å¯¹è¯æµç”Ÿæˆ
        5. å·¥å…·æ‰§è¡Œ
        6. ç»“æœæ”¶é›†

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            conversation_history: å¯¹è¯å†å²
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°

        Yields:
            æµå¼äº‹ä»¶å­—å…¸
        """
        # åˆå§‹åŒ–æµç”Ÿæˆå™¨
        stream_gen = StreamGenerator(
            session_id=context.session_id,
            format_sse=False,  # è¿”å›å­—å…¸æ ¼å¼
        )

        # å‡†å¤‡æ¶ˆæ¯å†å²
        messages = conversation_history or []
        messages.append({
            "role": "user",
            "content": user_message,
        })

        # ä¸»å¾ªç¯
        for iteration in range(max_iterations):
            self._stats["iterations"] += 1

            logger.info(
                f"ä¸»å¾ªç¯ - è¿­ä»£ {iteration + 1}/{max_iterations}"
            )

            # å‘é€è¿­ä»£äº‹ä»¶
            yield {
                "type": StreamEventType.ITERATION,
                "data": {
                    "iteration": iteration + 1,
                    "max_iterations": max_iterations,
                }
            }

            # ============================================
            # Stage 1: æ¶ˆæ¯é¢„å¤„ç†
            # ============================================
            messages = await self._stage_preprocess_messages(messages, context)
            self._stats["messages_processed"] += len(messages)

            # ============================================
            # Stage 2: å‹ç¼©æ£€æŸ¥
            # ============================================
            messages, compressed = await self._stage_compression_check(
                messages, context, stream_gen
            )

            if compressed:
                self._stats["compressions"] += 1
                yield {
                    "type": StreamEventType.COMPRESSION_SUCCESS,
                    "data": {
                        "message_count": len(messages),
                    }
                }

            # ============================================
            # Stage 3: ç³»ç»Ÿæç¤ºç”Ÿæˆ
            # ============================================
            system_prompt = await self._stage_generate_system_prompt(
                context, self.tool_executor.tool_schemas
            )

            # ============================================
            # Stage 4: å¯¹è¯æµç”Ÿæˆ
            # ============================================
            assistant_message = None

            async for event in self._stage_conversation_stream(
                messages, context, system_prompt, stream_gen
            ):
                # è½¬å‘æµå¼äº‹ä»¶
                yield event

                # æ”¶é›†å®Œæ•´æ¶ˆæ¯
                if event["type"] == "message_complete":
                    assistant_message = event["data"]["message"]

            if assistant_message is None:
                logger.error("æœªæ”¶åˆ°åŠ©æ‰‹å“åº”")
                break

            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
            messages.append({
                "role": "assistant",
                "content": assistant_message["content"],
            })

            # ============================================
            # Stage 5: å·¥å…·æ‰§è¡Œ
            # ============================================
            tool_calls = self._extract_tool_calls(assistant_message)

            if not tool_calls:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ
                logger.info("æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ")
                yield {
                    "type": StreamEventType.DONE,
                    "data": {
                        "final_message": assistant_message,
                        "stats": self.get_stats(),
                    }
                }
                break

            # æ‰§è¡Œå·¥å…·
            tool_results = []

            async for event in self._stage_tool_execution(
                tool_calls, context, stream_gen
            ):
                yield event

                # æ”¶é›†å·¥å…·ç»“æœ
                if event["type"] == StreamEventType.TOOL_RESULT:
                    tool_results.append(event["data"])

            self._stats["tools_executed"] += len(tool_calls)

            # ============================================
            # Stage 6: ç»“æœæ”¶é›†
            # ============================================
            messages = await self._stage_collect_results(
                messages, tool_results
            )

            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if context.abort_signal:
                logger.warning("æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢æ‰§è¡Œ")
                yield {
                    "type": StreamEventType.WARNING,
                    "data": {
                        "message": "Execution aborted by user",
                    }
                }
                break

        # æœ€ç»ˆå®Œæˆäº‹ä»¶
        yield {
            "type": StreamEventType.LOOP_COMPLETE,
            "data": {
                "iterations": iteration + 1,
                "stats": self.get_stats(),
            }
        }

    # ============================================
    # 6 é˜¶æ®µå®ç°
    # ============================================

    async def _stage_preprocess_messages(
        self,
        messages: List[Dict[str, Any]],
        context: ExecutionContext,
    ) -> List[Dict[str, Any]]:
        """
        Stage 1: æ¶ˆæ¯é¢„å¤„ç†

        æ¸…ç†å’Œæ ‡å‡†åŒ–æ¶ˆæ¯æ ¼å¼

        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            context: æ‰§è¡Œä¸Šä¸‹æ–‡

        Returns:
            å¤„ç†åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        logger.debug(f"[Preprocess] å¤„ç† {len(messages)} æ¡æ¶ˆæ¯")

        # TODO: å®ç°æ¶ˆæ¯æ¸…ç†é€»è¾‘
        # - ç§»é™¤ç©ºæ¶ˆæ¯
        # - åˆå¹¶è¿ç»­çš„åŒè§’è‰²æ¶ˆæ¯
        # - æ ¼å¼æ ‡å‡†åŒ–

        return messages

    async def _stage_compression_check(
        self,
        messages: List[Dict[str, Any]],
        context: ExecutionContext,
        stream_gen: StreamGenerator,
    ) -> tuple[List[Dict[str, Any]], bool]:
        """
        Stage 2: å‹ç¼©æ£€æŸ¥

        æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©æ¶ˆæ¯å†å²

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            stream_gen: æµç”Ÿæˆå™¨

        Returns:
            (å¤„ç†åçš„æ¶ˆæ¯åˆ—è¡¨, æ˜¯å¦æ‰§è¡Œäº†å‹ç¼©)
        """
        if not context.should_compress():
            return messages, False

        logger.info("å¼€å§‹æ¶ˆæ¯å‹ç¼©...")

        # å‘é€å‹ç¼©å¼€å§‹äº‹ä»¶
        await stream_gen.stream_compression(status="start")

        try:
            # æ‰§è¡Œå‹ç¼©
            compressed_messages, compressed = await self.compressor.compress_if_needed(
                messages, context
            )

            if compressed:
                logger.info(
                    f"å‹ç¼©å®Œæˆï¼š{len(messages)} -> {len(compressed_messages)} æ¡æ¶ˆæ¯"
                )

            return compressed_messages, compressed

        except Exception as e:
            logger.error(f"å‹ç¼©å¤±è´¥ï¼š{e}", exc_info=True)
            await stream_gen.stream_compression(status="failed", error=str(e))
            return messages, False

    async def _stage_generate_system_prompt(
        self,
        context: ExecutionContext,
        tool_schemas: Dict[str, Any],
    ) -> str:
        """
        Stage 3: ç³»ç»Ÿæç¤ºç”Ÿæˆ

        åŠ¨æ€ç”Ÿæˆç³»ç»Ÿæç¤º

        Args:
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            tool_schemas: å·¥å…· Schema å­—å…¸

        Returns:
            ç³»ç»Ÿæç¤ºæ–‡æœ¬
        """
        logger.debug("[System Prompt] ç”Ÿæˆç³»ç»Ÿæç¤º")

        tools_list = list(tool_schemas.values())

        system_prompt = self.prompt_generator.generate(
            context=context,
            tools=tools_list,
            include_subagent_info=True,
            include_compression_info=context.is_compressed,
        )

        return system_prompt

    async def _stage_conversation_stream(
        self,
        messages: List[Dict[str, Any]],
        context: ExecutionContext,
        system_prompt: str,
        stream_gen: StreamGenerator,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Stage 4: å¯¹è¯æµç”Ÿæˆ

        è°ƒç”¨ LLM ç”Ÿæˆå“åº”

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            system_prompt: ç³»ç»Ÿæç¤º
            stream_gen: æµç”Ÿæˆå™¨

        Yields:
            æµå¼äº‹ä»¶
        """
        logger.debug("[Conversation] å¼€å§‹ç”Ÿæˆå“åº”")

        async for event in self.conversation_pipeline.stream_conversation(
            messages=messages,
            context=context,
            system_prompt=system_prompt,
            tools=list(self.tool_executor.tool_schemas.values()),
        ):
            # è½¬æ¢ä¸ºç»Ÿä¸€äº‹ä»¶æ ¼å¼
            yield {
                "type": self._map_api_event_type(event.get("type")),
                "data": event,
            }

    def _map_api_event_type(self, api_event_type: str) -> str:
        """
        æ˜ å°„ API äº‹ä»¶ç±»å‹åˆ°å†…éƒ¨äº‹ä»¶ç±»å‹

        Args:
            api_event_type: API äº‹ä»¶ç±»å‹

        Returns:
            å†…éƒ¨äº‹ä»¶ç±»å‹
        """
        mapping = {
            "content_block_delta": StreamEventType.TEXT_DELTA,
            "message_complete": "message_complete",
            "message_start": "message_start",
        }

        return mapping.get(api_event_type, api_event_type)

    async def _stage_tool_execution(
        self,
        tool_calls: List[ToolCall],
        context: ExecutionContext,
        stream_gen: StreamGenerator,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Stage 5: å·¥å…·æ‰§è¡Œ

        å¹¶å‘æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨

        Args:
            tool_calls: å·¥å…·è°ƒç”¨åˆ—è¡¨
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            stream_gen: æµç”Ÿæˆå™¨

        Yields:
            æµå¼äº‹ä»¶
        """
        logger.info(f"[Tool Execution] æ‰§è¡Œ {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")

        # ä¸ºæ¯ä¸ªå·¥å…·è°ƒç”¨åˆ›å»ºä»»åŠ¡
        for tool_call in tool_calls:
            # å‘é€å·¥å…·æ‰§è¡Œå¼€å§‹äº‹ä»¶
            yield {
                "type": StreamEventType.TOOL_EXECUTING,
                "data": {
                    "tool_name": tool_call.name,
                    "tool_input": tool_call.input,
                    "call_id": tool_call.call_id,
                }
            }

            # è°ƒåº¦ä»»åŠ¡
            await self.concurrency_scheduler.schedule(
                self.tool_executor.execute,
                tool_call,
                context,
                priority=TaskPriority.HIGH,
                task_id=tool_call.call_id,
            )

        # æ‰§è¡Œæ‰€æœ‰å¾…å¤„ç†ä»»åŠ¡
        results = await self.concurrency_scheduler.execute_pending()

        # å‘é€å·¥å…·ç»“æœäº‹ä»¶
        for i, result in enumerate(results):
            tool_call = tool_calls[i]

            if isinstance(result, Exception):
                # æ‰§è¡Œå¤±è´¥
                yield {
                    "type": StreamEventType.TOOL_RESULT,
                    "data": {
                        "tool_name": tool_call.name,
                        "call_id": tool_call.call_id,
                        "success": False,
                        "error": str(result),
                    }
                }
            else:
                # æ‰§è¡ŒæˆåŠŸ
                yield {
                    "type": StreamEventType.TOOL_RESULT,
                    "data": {
                        "tool_name": tool_call.name,
                        "call_id": tool_call.call_id,
                        "success": result.success,
                        "result": result.result,
                    }
                }

    async def _stage_collect_results(
        self,
        messages: List[Dict[str, Any]],
        tool_results: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """
        Stage 6: ç»“æœæ”¶é›†

        å°†å·¥å…·æ‰§è¡Œç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²

        ç‰¹æ®Šå¤„ç†ï¼š
        - æ£€æµ‹ spawn_section_workers ç»“æœ
        - å¦‚æœ Workers å®Œæˆï¼Œå¼ºåˆ¶æ·»åŠ éªŒè¯æé†’
        - æ£€æµ‹ is_task_complete å­—æ®µï¼Œæ³¨å…¥åç»­æ­¥éª¤æç¤º

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tool_results: å·¥å…·ç»“æœåˆ—è¡¨

        Returns:
            æ›´æ–°åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        logger.debug(f"[Collect Results] æ”¶é›† {len(tool_results)} ä¸ªå·¥å…·ç»“æœ")

        # æ ‡è®°æ˜¯å¦éœ€è¦æ·»åŠ éªŒè¯æé†’
        needs_verification_reminder = False

        # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯å†å²
        for tool_result in tool_results:
            result_content = str(tool_result.get("result", ""))
            tool_name = tool_result.get("tool_name", "")

            messages.append({
                "role": "tool",
                "tool_use_id": tool_result.get("call_id"),
                "content": result_content,
            })

            # æ£€æµ‹ spawn_section_workers ç»“æœ
            # å¦‚æœåŒ…å« WORKERS_COMPLETED æˆ– is_task_complete: Falseï¼Œéœ€è¦å¼ºåˆ¶éªŒè¯
            if tool_name == "spawn_section_workers":
                logger.info("[Collect Results] æ£€æµ‹åˆ° spawn_section_workers å®Œæˆï¼Œæ·»åŠ éªŒè¯æé†’")
                needs_verification_reminder = True
            elif "WORKERS_COMPLETED" in result_content or "is_task_complete" in result_content:
                logger.info("[Collect Results] æ£€æµ‹åˆ° Worker å®Œæˆæ ‡è®°ï¼Œæ·»åŠ éªŒè¯æé†’")
                needs_verification_reminder = True

        # å¦‚æœéœ€è¦éªŒè¯æé†’ï¼Œæ·»åŠ ç³»ç»Ÿæ¶ˆæ¯å¼ºè°ƒåç»­æ­¥éª¤
        if needs_verification_reminder:
            verification_reminder = (
                "\n\nğŸš¨ SYSTEM REMINDER: Workers have completed, but the task is NOT done!\n\n"
                "You MUST execute these steps NOW:\n"
                "1. Wait 3-5 seconds for HMR to reload new files\n"
                "2. get_build_errors() - Check for compilation errors!\n"
                "3. If errors â†’ fix them â†’ get_build_errors() again\n\n"
                "â›” DO NOT call shell('npm run dev') - Dev server is already running!\n"
                "â›” DO NOT generate a final response until you have checked for errors!"
            )

            # åœ¨æœ€åä¸€ä¸ªå·¥å…·ç»“æœåè¿½åŠ æé†’
            if messages and messages[-1].get("role") == "tool":
                messages[-1]["content"] += verification_reminder
                logger.info("[Collect Results] éªŒè¯æé†’å·²æ·»åŠ åˆ°å·¥å…·ç»“æœ")

        return messages

    # ============================================
    # è¾…åŠ©æ–¹æ³•
    # ============================================

    def _extract_tool_calls(
        self,
        assistant_message: Dict[str, Any]
    ) -> List[ToolCall]:
        """
        ä»åŠ©æ‰‹æ¶ˆæ¯ä¸­æå–å·¥å…·è°ƒç”¨

        Args:
            assistant_message: åŠ©æ‰‹æ¶ˆæ¯

        Returns:
            å·¥å…·è°ƒç”¨åˆ—è¡¨
        """
        tool_calls = []

        content = assistant_message.get("content", [])

        for block in content:
            if isinstance(block, dict) and block.get("type") == "tool_use":
                tool_call = ToolCall(
                    name=block.get("name"),
                    input=block.get("input", {}),
                    call_id=block.get("id"),
                )
                tool_calls.append(tool_call)

        return tool_calls

    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–ç¼–æ’å™¨ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡æ•°æ®å­—å…¸
        """
        return {
            **self._stats,
            "message_queue": self.message_queue.get_stats(),
            "conversation": self.conversation_pipeline.get_stats(),
            "tool_executor": self.tool_executor.get_stats(),
            "scheduler": self.concurrency_scheduler.get_stats(),
        }

    def __repr__(self) -> str:
        stats = self.get_stats()
        return (
            f"MainOrchestrator("
            f"iterations={stats['iterations']}, "
            f"messages={stats['messages_processed']}, "
            f"tools={stats['tools_executed']})"
        )
